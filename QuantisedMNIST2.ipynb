{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,32,3,1,1)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.conv2=nn.Conv2d(32,64,3,1,1)\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1=nn.Linear(in_features=12544,out_features=128)\n",
    "        self.relu3=nn.ReLU()\n",
    "        self.fc2=nn.Linear(in_features=128,out_features=10)\n",
    "        self.log_softmax=nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.relu1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.pool(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu3(x)\n",
    "        x=self.fc2(x)\n",
    "        output=self.log_softmax(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def fmevaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end =\"\")\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def qmevaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end =\"\")\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def load_model(model_file):\n",
    "    model = BaseModel()\n",
    "    state_dict = torch.load(model_file, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 30\n",
    "eval_batch_size = 50\n",
    "\n",
    "def prepare_data_loaders():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_dataset)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = prepare_data_loaders()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\princ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "float_model_file = 'base_model.pth'\n",
    "scripted_float_model_file = 'float_model_scripted.pth'\n",
    "scripted_quantized_model_file = 'quantized_model_scripted.pth'\n",
    "\n",
    "float_model = load_model(float_model_file).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_model.eval()\n",
    "float_model_fused = torch.ao.quantization.fuse_modules(float_model, [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 6.506268\n",
      "........................................................................................................................................................................................................\n",
      "Evaluation accuracy on 50000 images, 96.98\n"
     ]
    }
   ],
   "source": [
    "float_model_file = 'base_model.pth'\n",
    "\n",
    "num_eval_batches = 1000\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "top1, top5 = fmevaluate(float_model, criterion, test_loader, neval_batches=num_eval_batches)\n",
    "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), scripted_float_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(QuantizedModel, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.model = model\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# Assuming per_channel_quantized_model is already loaded\n",
    "per_channel_quantized_model = load_model(float_model_file)\n",
    "\n",
    "per_channel_quantized_model.log_softmax = None\n",
    "def modified_forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.relu1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.pool(x)\n",
    "        x=self.dropout(x)\n",
    "        x=self.flatten(x)\n",
    "        x=self.fc1(x)\n",
    "        x=self.relu3(x)\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "per_channel_quantized_model.forward = modified_forward.__get__(per_channel_quantized_model, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\princ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizedModel(\n",
      "  (quant): Quantize(scale=tensor([0.0255]), zero_point=tensor([17]), dtype=torch.quint8)\n",
      "  (model): BaseModel(\n",
      "    (conv1): QuantizedConvReLU2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.036530353128910065, zero_point=0, padding=(1, 1))\n",
      "    (relu1): Identity()\n",
      "    (conv2): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.04108048230409622, zero_point=0, padding=(1, 1))\n",
      "    (relu2): Identity()\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout): QuantizedDropout(p=0.5, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): QuantizedLinearReLU(in_features=12544, out_features=128, scale=0.18267779052257538, zero_point=0, qscheme=torch.per_channel_affine)\n",
      "    (relu3): Identity()\n",
      "    (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.2601340711116791, zero_point=61, qscheme=torch.per_channel_affine)\n",
      "    (log_softmax): None\n",
      "  )\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Fuse the modules in the original model\n",
    "fused_model = torch.ao.quantization.fuse_modules(per_channel_quantized_model, [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3']])\n",
    "\n",
    "# Create a modified model with the fused original model\n",
    "quantized_model = QuantizedModel(fused_model)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "quantized_model.eval()\n",
    "\n",
    "# Set the quantization configuration\n",
    "quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "# Prepare the model for quantization\n",
    "torch.ao.quantization.prepare(quantized_model, inplace=True)\n",
    "\n",
    "# Calibrate the model with representative data\n",
    "for data, _ in train_loader:\n",
    "    quantized_model(data)\n",
    "\n",
    "# Convert the model to a quantized version\n",
    "torch.ao.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "# Print the modified model architecture\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of quantized model\n",
      "Size (MB): 1.637678\n",
      "........................................................................................................................................................................................................\n",
      "Evaluation accuracy on 50000 images, 96.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of quantized model\")\n",
    "print_size_of_model(quantized_model)\n",
    "top1, top5 = qmevaluate(quantized_model, criterion, test_loader, neval_batches=num_eval_batches)\n",
    "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.script(quantized_model), scripted_quantized_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Size :\n",
      "Size (MB): 6.506268\n",
      "Quantized Model Size :\n",
      "Size (MB): 1.637678\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Model Size :\")\n",
    "print_size_of_model(float_model)\n",
    "print(\"Quantized Model Size :\")\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Accuracy :\n",
      "........................................................................................................................................................................................................\n",
      "Evaluation accuracy on 50000 images, 96.98\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quantized Model Accuracy :\n",
      "........................................................................................................................................................................................................\n",
      "Evaluation accuracy on 50000 images, 96.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Model Accuracy :\")\n",
    "top1, top5 = fmevaluate(float_model, criterion, test_loader, neval_batches=num_eval_batches)\n",
    "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"Quantized Model Accuracy :\")\n",
    "top1, top5 = qmevaluate(quantized_model, criterion, test_loader, neval_batches=num_eval_batches)\n",
    "print('\\nEvaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model average inference time: 0.000971 seconds\n",
      "Quantized model average inference time: 0.000443 seconds\n"
     ]
    }
   ],
   "source": [
    "def measure_inference_time(model, input_data, num_iterations=100):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Warm up the model (optional but recommended for more accurate timing)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(input_data)\n",
    "    \n",
    "    # Measure the inference time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            _ = model(input_data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_inference_time = (end_time - start_time) / num_iterations\n",
    "    return avg_inference_time\n",
    "\n",
    "# Assuming float_model and modified_model are already defined and loaded\n",
    "# Create some dummy input data\n",
    "input_data = torch.randn(1, 1, 28, 28)  # Adjust the shape according to your model's input\n",
    "\n",
    "# Measure inference time for the float model\n",
    "float_model_inference_time = measure_inference_time(float_model, input_data)\n",
    "print(f\"Float model average inference time: {float_model_inference_time:.6f} seconds\")\n",
    "\n",
    "# Measure inference time for the quantized model\n",
    "quantized_model_inference_time = measure_inference_time(quantized_model, input_data)\n",
    "print(f\"Quantized model average inference time: {quantized_model_inference_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that the Quantized Model offers a significant speed boost, consumes less memory, and doesn't compromise on accuracy. This follows the general case with quantization where we can maintain efficiency of a model while significantly reducing memory size requirements and inference time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
